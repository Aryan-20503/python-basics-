{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1 : What is Information Gain, and how is it used in Decision Trees?**\n",
        "Information Gain measures the reduction in uncertainty (entropy) about the target variable after splitting the dataset using a particular feature.\n",
        "\n",
        "- Higher Information Gain ⇒ Better Feature for Splitting\n",
        "\n",
        "- Goal: Choose the feature that results in pure child nodes (i.e., nodes dominated by a single class)\n",
        "\n",
        "HOW IT IS USED IN DECISION TREE\n",
        "| Step | Role of Information Gain                                  |\n",
        "| ---- | --------------------------------------------------------- |\n",
        "| 1️⃣  | Check all features and calculate IG for each              |\n",
        "| 2️⃣  | Choose the feature with **highest IG** to make a split    |\n",
        "| 3️⃣  | Repeat recursively for sub-nodes until stopping condition |\n"
      ],
      "metadata": {
        "id": "dY59YcHSFEyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "(***Hint: Directly compares the two main impurity measures, highlighting strengths,weaknesses, and appropriate use cases.)***\n",
        "\n",
        "Difference Between Gini Impurity and Entropy\n",
        "| Feature                   | **Gini Impurity**                                                        | **Entropy**                                                   |\n",
        "| ------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------- |\n",
        "| **Definition**            | Measures likelihood of incorrectly classifying a randomly chosen element | Measures the amount of disorder or uncertainty in the dataset |\n",
        "| **Formula**               | ( \\text{Gini} = 1 - \\sum p_i^2 )                                         | ( \\text{Entropy} = - \\sum p_i \\log_2 p_i )                    |\n",
        "| **Interpretation**        | Focuses on misclassification probability                                 | Focuses on information content (bit-based measure)            |\n",
        "| **Range**                 | 0 (pure) to ~0.5 (binary max impurity)                                   | 0 (pure) to 1 (binary max impurity)                           |\n",
        "| **Computation Speed**     | **Faster** — no logarithm involved                                       | **Slower** — uses logarithms                                  |\n",
        "| **Bias During Splitting** | Prefers **larger** partitions (more stable splits)                       | Prefers splits that create **purer** smaller subsets          |\n",
        "| **Commonly Used In**      | CART (Classification & Regression Trees)                                 | ID3, C4.5, C5.0 decision trees                                |\n",
        "| **Best Use Case**         | Larger datasets and real-time models                                     | When purity matters more than speed                           |\n",
        "\n"
      ],
      "metadata": {
        "id": "ffDyTlFHFEvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 3:What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "Pre-Pruning in Decision Trees\n",
        "\n",
        "Pre-pruning (also called early stopping) is a technique used to stop the growth of a decision tree before it becomes too complex and starts overfitting the training data.\n",
        "\n",
        "How It Works?\n",
        "\n",
        "During tree construction, the algorithm evaluates splits at each node.\n",
        "If a split does not significantly improve model performance, the algorithm:\n",
        "\n",
        "-  Stops further splitting\n",
        "-  Converts that node into a leaf node\n",
        "-  Prevents over-complex branches\n",
        "\n",
        "Common Pre-Pruning Criteria\n",
        "\n",
        "A tree stops splitting if:\n",
        "| Condition                                          | Meaning                                                |\n",
        "| -------------------------------------------------- | ------------------------------------------------------ |\n",
        "| **Minimum samples per node**                       | If too few samples, stop splitting                     |\n",
        "| **Maximum tree depth**                             | Tree cannot grow beyond a set depth                    |\n",
        "| **Minimum information gain or impurity reduction** | Split must sufficiently improve purity                 |\n",
        "| **Validation set performance**                     | If performance doesn’t improve, prevent further splits |\n",
        "\n",
        "PURPOSE\n",
        "| Goal                     | Benefit                        |\n",
        "| ------------------------ | ------------------------------ |\n",
        "| Reduce overfitting       | Improves generalization        |\n",
        "| Control model complexity | Faster training and prediction |\n",
        "| Avoid useless branches   | Better interpretability        |\n"
      ],
      "metadata": {
        "id": "6Z-V0PIZFErg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).**\n",
        "\n",
        "***(Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_. (Include your Python code and output in the code box below.))***\n",
        "\n",
        "This demonstrates:\n",
        "\n",
        "-  Decision Tree using Gini Impurity\n",
        "-  Training on a dataset\n",
        "-  Printing feature importances clearly"
      ],
      "metadata": {
        "id": "voBJj6j9FEo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4: Decision Tree Classifier using Gini Impurity\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Train Decision Tree with Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Display feature importances with labels\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN82TK5gKG0J",
        "outputId": "04c7b41f-c0ed-4116-b723-60db4ad73413"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "Support Vector Machine (SVM)\n",
        "\n",
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks, but most commonly for binary classification.\n",
        "\n",
        "Core Idea:\n",
        "\n",
        "SVM tries to find the best separating boundary (called hyperplane) between classes.\n",
        "\n",
        "That hyperplane is chosen such that:\n",
        "\n",
        "- It maximizes the margin — the distance between the hyperplane and the nearest data points from each class\n",
        "-  These critical nearest points are called Support Vectors (hence the name)\n",
        "\n",
        "KEY CONCEPT\n",
        "| Term                | Meaning                                                      |\n",
        "| ------------------- | ------------------------------------------------------------ |\n",
        "| **Hyperplane**      | Decision boundary separating classes                         |\n",
        "| **Margin**          | Distance between hyperplane and support vectors              |\n",
        "| **Support Vectors** | Closest data points that influence the boundary              |\n",
        "| **Kernel Trick**    | Maps data to higher dimensions to make it linearly separable |\n",
        "\n",
        "Kernels in SVM\n",
        "\n",
        "| Kernel         | Use Case                                     |\n",
        "| -------------- | -------------------------------------------- |\n",
        "| Linear         | Data is linearly separable                   |\n",
        "| Polynomial     | Complex boundaries with polynomial patterns  |\n",
        "| RBF (Gaussian) | Most common; handles nonlinear relationships |\n",
        "| Sigmoid        | Neural network–like behavior                 |\n",
        "\n",
        "WHY SVM?\n",
        "| Strengths                             | Weaknesses                                 |\n",
        "| ------------------------------------- | ------------------------------------------ |\n",
        "| Works well with high-dimensional data | Slow for large datasets                    |\n",
        "| Effective for clear margin separation | Sensitive to noise and overlapping classes |\n",
        "| Flexible via kernels                  | Requires parameter tuning                  |\n"
      ],
      "metadata": {
        "id": "MDpvgq7QFElL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: What is the Kernel Trick in SVM?**\n",
        "\n",
        "Kernel Trick in SVM\n",
        "\n",
        "The Kernel Trick is a mathematical technique used in Support Vector Machines that allows us to:\n",
        "\n",
        "-  Handle non-linear data\n",
        "-  Without explicitly transforming data into a higher dimension\n",
        "\n",
        "What Problem Does It Solve?\n",
        "\n",
        "Many datasets cannot be separated by a straight line in the original input space.\n",
        "\n",
        "Example:\n",
        "Classes shaped like concentric circles → linear SVM fails\n",
        "\n",
        "So, SVM maps data into a higher-dimensional space where the classes become linearly separable.\n",
        "\n",
        "But explicitly transforming data (e.g., from 2D → 3D → 100D) is:\n",
        "\n",
        "-  Very expensive\n",
        "-  Sometimes impossible to compute\n",
        "\n",
        "The Kernel Trick Solution\n",
        "\n",
        "Instead of computing the actual transformation,\n",
        "SVM uses a kernel function that calculates the dot product in the high-dimensional space directly from the original space.\n",
        "\n",
        "Thus:\n",
        "\n",
        "Complex classification boundaries are learned efficiently without heavy computation.\n",
        "\n",
        "COMMON KERNEL FUNCTIONS\n",
        "| Kernel             | When to Use                                      |\n",
        "| ------------------ | ------------------------------------------------ |\n",
        "| **Linear**         | Data is already linearly separable               |\n",
        "| **Polynomial**     | Data has polynomial relationships                |\n",
        "| **RBF (Gaussian)** | Most common; handles complex non-linear patterns |\n",
        "| **Sigmoid**        | Similar to neural network activation             |\n"
      ],
      "metadata": {
        "id": "W10QalVkLx8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.**\n",
        "\n",
        "***Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset.***\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n"
      ],
      "metadata": {
        "id": "bU12uzOXMjhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: SVM with Linear & RBF Kernels - Accuracy Comparison\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Train-Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(\"Accuracy Comparison:\")\n",
        "print(f\"Linear Kernel SVM Accuracy: {acc_linear:.4f}\")\n",
        "print(f\"RBF Kernel SVM Accuracy: {acc_rbf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KOcoD_PNJny",
        "outputId": "2cfa77bb-2e2e-4939-80c9-1a0b37b5948b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison:\n",
            "Linear Kernel SVM Accuracy: 0.9815\n",
            "RBF Kernel SVM Accuracy: 0.7593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "\n",
        "Naïve Bayes Classifier\n",
        "\n",
        "Naïve Bayes is a probabilistic, supervised machine learning algorithm based on Bayes’ Theorem.\n",
        "It is widely used for classification tasks, especially in:\n",
        "\n",
        "- Spam detection\n",
        "\n",
        "- Sentiment analysis\n",
        "\n",
        "- Text classification\n",
        "\n",
        "- Medical diagnosis\n",
        "\n",
        "How Does It Work?\n",
        "\n",
        "It calculates the probability of each class for a given feature set and assigns the class with the maximum posterior probability.\n",
        "\n",
        "P(Class∣Features)=\n",
        "P(Features∣Class)⋅P(Class) / P(Features)\t​\n",
        "\n",
        "KEY CHARACTERISTICS\n",
        "| Feature         | Description                                 |\n",
        "| --------------- | ------------------------------------------- |\n",
        "| Assumption      | Features are independent (the \"naïve\" part) |\n",
        "| Efficiency      | Very fast training and prediction           |\n",
        "| Works well with | High-dimensional data (e.g., text)          |\n",
        "| Output          | Class with highest probability              |\n",
        "\n",
        "Advantages\n",
        "\n",
        "- Simple and fast\n",
        "\n",
        "- Works surprisingly well even when independence assumption is violated\n",
        "\n",
        "- Performs great in real-world text-based tasks\n",
        "\n",
        " Limitation\n",
        "\n",
        "- If features are highly correlated, performance may drop\n",
        "\n",
        "- Zero probability issue (handled using Laplace Smoothing)\n",
        "\n",
        "WHY IS IT CALLED NANIVE?\n",
        "\n",
        "Because it makes a strong and unrealistic assumption:\n",
        "\n",
        "***All features are conditionally independent of each other given the class label.***\n",
        "\n",
        "Example:\n",
        "In text classification, it assumes each word appears independently — which is not always true."
      ],
      "metadata": {
        "id": "fFlLCFcgMjdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**\n",
        "\n",
        "| Type of Naïve Bayes         | Assumption About Features                            | Suitable Data Type                | Common Use Cases                                    | Example Scenario                         |\n",
        "| --------------------------- | ---------------------------------------------------- | --------------------------------- | --------------------------------------------------- | ---------------------------------------- |\n",
        "| **Gaussian Naïve Bayes**    | Features follow a **normal (Gaussian) distribution** | Continuous / real-valued features | Classification with continuous measurements         | Height, weight, temperature, sensor data |\n",
        "| **Multinomial Naïve Bayes** | Features are **counts** or **frequencies** of events | Discrete numeric data (≥ 0)       | Text classification, document term frequency models | Bag-of-Words, TF-IDF values              |\n",
        "| **Bernoulli Naïve Bayes**   | Binary features: **0/1 presence or absence**         | Boolean indicators                | Spam detection with binary word occurrence          | Whether a specific word appears (yes/no) |\n",
        "\n",
        "\n",
        "Key Points Summary\n",
        "\n",
        "Gaussian NB\n",
        "\n",
        "-  Best for continuous features\n",
        "-  Assumes data fits a bell-curve distribution\n",
        "\n",
        "Multinomial NB\n",
        "\n",
        "-  Best for text with word frequency data\n",
        "-  More sensitive to number of occurrences\n",
        "\n",
        "Bernoulli NB\n",
        "-  Best for binary features (present vs absent)\n",
        "-  Evaluates whether a feature exists, not how many times\n",
        "\n",
        "Final Understanding\n",
        "\n",
        "***The core difference lies in the type of data each variant handles (continuous vs count vs binary), and the probability distribution assumed for the features.***"
      ],
      "metadata": {
        "id": "pXatTjn1Lxg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: Breast Cancer Dataset**\n",
        "\n",
        "**Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.**\n",
        "\n",
        "***Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.***\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n"
      ],
      "metadata": {
        "id": "0SX4QwVvLxSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Question 10: Gaussian Naïve Bayes on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into Train & Test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train the Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy on Breast Cancer Dataset:\")\n",
        "print(f\"Gaussian Naïve Bayes Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1Sx5C8tQHxY",
        "outputId": "2c4c0cb0-cc4f-4234-ff2f-70d818220335"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy on Breast Cancer Dataset:\n",
            "Gaussian Naïve Bayes Accuracy: 0.9415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OCWEGvzgLxLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kPmTpw76LxIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDjLWWrvE1n0"
      },
      "outputs": [],
      "source": []
    }
  ]
}